{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe27e33f",
   "metadata": {},
   "source": [
    "# Conceptual and Statistical Introduction\n",
    "\n",
    "## The statistical problem of high dimensionality\n",
    "\n",
    "In high-dimensional biological data, many features are correlated. Variance measures how spread out data points are around their mean. Covariance measures how features vary together.\n",
    "\n",
    "## Geometric intuition\n",
    "\n",
    "PCA rotates the coordinate system to align with directions of maximum variance. These directions correspond to eigenvectors of the covariance matrix and represent the most informative axes of variation.\n",
    "\n",
    "## Drug development relevance\n",
    "\n",
    "Omics datasets often contain thousands of variables but few samples. PCA helps separate signal from noise and identify dominant biological processes before downstream biomarker analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7113c747",
   "metadata": {},
   "source": [
    "# PCA for Biomarker Discovery (Drug Development)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b9e64d",
   "metadata": {},
   "source": [
    "## 1. Motivation\n",
    "\n",
    "High-dimensional biological data contains correlated features. PCA reduces dimensionality while preserving dominant variance patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c81594d",
   "metadata": {},
   "source": [
    "## 2. Generating Synthetic Biomarker Data\n",
    "\n",
    "Rows represent samples and columns represent genes or proteins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3d0114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# NumPy is used for numerical computation and random number generation\n",
    "\n",
    "import pandas as pd\n",
    "# Pandas provides labeled DataFrames, which are critical for tracking gene identities\n",
    "\n",
    "np.random.seed(42)\n",
    "# Fixing the random seed ensures reproducibility of the synthetic dataset\n",
    "# Changing this value would generate a different dataset and slightly alter PCA results\n",
    "\n",
    "num_samples = 50\n",
    "# Number of biological samples (e.g., patients or experiments)\n",
    "# In omics, samples are often far fewer than features\n",
    "\n",
    "num_features = 100\n",
    "# Number of genes or proteins measured per sample\n",
    "# High feature dimensionality motivates dimensionality reduction\n",
    "\n",
    "data = np.random.rand(num_samples, num_features) * 10\n",
    "# np.random.rand generates values in the range [0, 1)\n",
    "# Multiplying by 10 increases variance magnitude to mimic real biological measurements\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    data,\n",
    "    columns=[f'Gene_{i+1}' for i in range(num_features)]\n",
    ")\n",
    "# Assign explicit gene labels so PCA loadings remain interpretable\n",
    "\n",
    "df.head()\n",
    "# Display the first few rows to verify data structure and scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed458b2c",
   "metadata": {},
   "source": [
    "## 3. Standardizing the Data\n",
    "\n",
    "PCA is sensitive to feature scale. Standardization ensures equal contribution from all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0188a2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# StandardScaler centers features to mean = 0 and scales to unit variance\n",
    "# Without this step, high-magnitude genes would dominate PCA directions\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# Initializes the scaler object, which will learn mean and standard deviation\n",
    "\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "# fit_transform first learns scaling statistics, then applies them\n",
    "# Each gene now contributes equally to variance calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31aff246",
   "metadata": {},
   "source": [
    "## 4. Performing PCA\n",
    "\n",
    "Principal components capture orthogonal directions of maximum variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b25d73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# PCA performs eigen decomposition of the covariance matrix\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "# n_components determines how many principal directions are retained\n",
    "# Fewer components increase compression but lose information\n",
    "\n",
    "principal_components = pca.fit_transform(scaled_data)\n",
    "# fit_transform learns principal axes and projects data onto them\n",
    "\n",
    "pc_df = pd.DataFrame(\n",
    "    principal_components,\n",
    "    columns=[f'PC_{i+1}' for i in range(10)]\n",
    ")\n",
    "# Store principal components in a labeled DataFrame for interpretation\n",
    "\n",
    "pc_df.head()\n",
    "# Inspect the transformed low-dimensional representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992fba38",
   "metadata": {},
   "source": [
    "## 5. Explained Variance Analysis\n",
    "\n",
    "Explained variance indicates how much information each component retains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7967483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Matplotlib is used for visualizing variance trends\n",
    "\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "# Fraction of total variance explained by each principal component\n",
    "\n",
    "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "# Cumulative sum shows how variance accumulates as more components are added\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "# figsize controls plot dimensions for readability\n",
    "\n",
    "plt.plot(\n",
    "    range(1, len(explained_variance_ratio)+1),\n",
    "    explained_variance_ratio,\n",
    "    'o-',\n",
    "    label='Individual'\n",
    ")\n",
    "# Individual variance per component\n",
    "\n",
    "plt.plot(\n",
    "    range(1, len(explained_variance_ratio)+1),\n",
    "    cumulative_explained_variance,\n",
    "    'o-',\n",
    "    label='Cumulative'\n",
    ")\n",
    "# Cumulative variance guides how many components to retain\n",
    "\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# Visualize variance retention to inform dimensionality choice"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

