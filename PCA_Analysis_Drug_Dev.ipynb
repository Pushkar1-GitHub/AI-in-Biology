{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conceptual and Statistical Introduction\n",
    "\n",
    "## The statistical problem of high dimensionality\n",
    "\n",
    "In high-dimensional biological data, many features are correlated. Variance measures how spread out data points are around their mean. Covariance measures how features vary together.\n",
    "\n",
    "## Geometric intuition\n",
    "\n",
    "PCA rotates the coordinate system to align with directions of maximum variance. These directions correspond to eigenvectors of the covariance matrix and represent the most informative axes of variation.\n",
    "\n",
    "## Drug development relevance\n",
    "\n",
    "Omics datasets often contain thousands of variables but few samples. PCA helps separate signal from noise and identify dominant biological processes before downstream biomarker analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuitive Understanding of PCA (Feynman-Style)\n",
    "\n",
    "Before applying PCA to real biological data, we build intuition using a very small, concrete example.\n",
    "\n",
    "**PCA asks a simple question:**\n",
    "\n",
    "> In which direction are the data points most spread out?\n",
    "\n",
    "That direction captures the most information and becomes the **first principal component**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Simple Numeric Example\n",
    "\n",
    "Consider the following four points:\n",
    "\n",
    "- (1, 2)\n",
    "- (2, 4)\n",
    "- (3, 6)\n",
    "- (4, 8)\n",
    "\n",
    "As x increases by 1, y increases by 2. The points lie almost perfectly along a straight line.\n",
    "\n",
    "Intuitively, almost all information lies along **one direction**, not two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a small 2D dataset with strong correlation\n",
    "X = np.array([\n",
    "    [1, 2],\n",
    "    [2, 4],\n",
    "    [3, 6],\n",
    "    [4, 8]\n",
    "])\n",
    "\n",
    "# Plot the original data points\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Original Data Points')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Mean-Centering Is Necessary\n",
    "\n",
    "PCA measures variance **around the mean**.\n",
    "\n",
    "If the data cloud is not centered at the origin, distances are measured incorrectly.\n",
    "\n",
    "Mean-centering shifts the cloud so its center lies at (0, 0), without changing its shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean of each feature\n",
    "mean = np.mean(X, axis=0)\n",
    "\n",
    "# Center the data\n",
    "X_centered = X - mean\n",
    "\n",
    "# Plot mean-centered data\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X_centered[:, 0], X_centered[:, 1])\n",
    "plt.axhline(0, color='gray', linestyle='--')\n",
    "plt.axvline(0, color='gray', linestyle='--')\n",
    "plt.xlabel('Centered Feature 1')\n",
    "plt.ylabel('Centered Feature 2')\n",
    "plt.title('Mean-Centered Data')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direction of Maximum Variance\n",
    "\n",
    "The direction along which the projected points are most spread out captures the maximum variance.\n",
    "\n",
    "In this example, that direction aligns closely with the line y = 2x.\n",
    "\n",
    "PCA mathematically finds this direction by maximizing variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA for Biomarker Discovery (Drug Development)\n",
    "\n",
    "## 1. Motivation\n",
    "\n",
    "High-dimensional biological data contains correlated features. PCA reduces dimensionality while preserving dominant variance patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generating Synthetic Biomarker Data\n",
    "\n",
    "Rows represent samples and columns represent genes or proteins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "num_samples = 50\n",
    "num_features = 100\n",
    "\n",
    "data = np.random.rand(num_samples, num_features) * 10\n",
    "\n",
    "df = pd.DataFrame(data, columns=[f'Gene_{i+1}' for i in range(num_features)])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Standardizing the Data\n",
    "\n",
    "PCA is sensitive to feature scale. Standardization ensures equal contribution from all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performing PCA\n",
    "\n",
    "Principal components capture orthogonal directions of maximum variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "principal_components = pca.fit_transform(scaled_data)\n",
    "\n",
    "pc_df = pd.DataFrame(principal_components, columns=[f'PC_{i+1}' for i in range(10)])\n",
    "\n",
    "pc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Explained Variance Analysis\n",
    "\n",
    "Explained variance indicates how much information each component retains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(explained_variance_ratio)+1), explained_variance_ratio, 'o-', label='Individual')\n",
    "plt.plot(range(1, len(explained_variance_ratio)+1), cumulative_explained_variance, 'o-', label='Cumulative')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
