{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe27e33f",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA) for Biomarker Discovery in Drug Development\n",
    "\n",
    "## Deep Conceptual Foundation\n",
    "\n",
    "### The Fundamental Problem: Curse of Dimensionality in Biological Data\n",
    "\n",
    "In modern drug development, biological measurements produce **high-dimensional data**: gene expression arrays measure 20,000+ genes, proteomics platforms quantify thousands of proteins, and metabolomics screens hundreds of metabolites. Yet clinical studies typically have only 50-500 samples due to cost and ethical constraints.\n",
    "\n",
    "This creates a severe **sample-to-feature ratio problem**:\n",
    "- **Overfitting**: Models memorize noise instead of learning true biological patterns\n",
    "- **Computational burden**: Distance calculations scale with O(n²) in high dimensions\n",
    "- **Statistical power loss**: Multiple testing correction becomes prohibitively strict\n",
    "- **Visualization impossibility**: Cannot plot or inspect data beyond 3 dimensions\n",
    "\n",
    "Moreover, many biological features are **correlated**:\n",
    "- Genes in the same pathway respond together\n",
    "- Proteins in the same complex are co-expressed\n",
    "- Metabolites in the same biochemical reaction share variation patterns\n",
    "\n",
    "This correlation means that the **effective dimensionality** is much lower than the measured dimensionality. PCA exploits this redundancy to compress information.\n",
    "\n",
    "### What is Variance and Why Does It Matter?\n",
    "\n",
    "**Variance** measures how spread out data points are around their mean:\n",
    "```\n",
    "Var(X) = (1/n) Σ(xᵢ - μ)²\n",
    "```\n",
    "\n",
    "High variance = large spread = high information content\n",
    "Low variance = clustered near mean = low information content\n",
    "\n",
    "**Covariance** measures how two features vary together:\n",
    "```\n",
    "Cov(X, Y) = (1/n) Σ(xᵢ - μₓ)(yᵢ - μᵧ)\n",
    "```\n",
    "\n",
    "Positive covariance = features increase together (correlated)\n",
    "Negative covariance = one increases as other decreases (anti-correlated)\n",
    "Zero covariance = independent variation\n",
    "\n",
    "In drug development:\n",
    "- **High variance features** separate responders from non-responders\n",
    "- **Low variance features** are often technical noise or housekeeping genes\n",
    "- **Correlated features** reveal biological modules or pathways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geometric_intuition_deep",
   "metadata": {},
   "source": [
    "## Geometric Intuition: Rotating to Find Maximum Variance\n",
    "\n",
    "### The Core Idea: Coordinate System Rotation\n",
    "\n",
    "Imagine you have 2D data (two gene measurements) scattered in an elliptical cloud:\n",
    "\n",
    "```\n",
    "Original axes (Gene1, Gene2):\n",
    "    Gene2 ↑\n",
    "          |\n",
    "       ● ●|● ●\n",
    "      ●  ●|●  ●\n",
    "     ●   ●●   ●\n",
    "    ●   ● | ●   ●\n",
    "   ●  ●   |   ●  ●\n",
    "  --------|----------> Gene1\n",
    "     ●  ● | ●  ●\n",
    "       ● ●|● ●\n",
    "```\n",
    "\n",
    "The data is correlated (ellipse tilted). Most variation occurs along the **long axis** of the ellipse, not along Gene1 or Gene2.\n",
    "\n",
    "**PCA rotates the axes** to align with the ellipse:\n",
    "\n",
    "```\n",
    "Rotated axes (PC1, PC2):\n",
    "        PC2\n",
    "         ↑ ●\n",
    "         |● ●\n",
    "        ●|●  ●\n",
    "       ● | ●  ●\n",
    "      ●  |  ●  ●\n",
    "     ●   ↗   ●   ● ← PC1 (direction of maximum variance)\n",
    "    ●  ●     ●  ●\n",
    "   ●  ●       ●  ●\n",
    "  ● ●           ● ●\n",
    " ●                 ●\n",
    "```\n",
    "\n",
    "**PC1** (first principal component) points along the direction where data spreads the most.\n",
    "**PC2** (second principal component) is perpendicular to PC1 and captures remaining variance.\n",
    "\n",
    "### Why This is Powerful\n",
    "\n",
    "After rotation:\n",
    "- **Most information is in PC1**: You can often discard PC2 with minimal information loss\n",
    "- **Components are uncorrelated**: Each PC captures independent variation\n",
    "- **Variance is ordered**: PC1 > PC2 > PC3 > ... by construction\n",
    "\n",
    "This lets you reduce 100 correlated genes to 10 independent principal components that capture 90% of the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toy_example_pca",
   "metadata": {},
   "source": [
    "## Concrete Numerical Example: Before vs After\n",
    "\n",
    "### BEFORE PCA (Original 3-Gene Data for 4 Samples)\n",
    "\n",
    "Imagine we measure 3 genes in 4 patient samples:\n",
    "\n",
    "```\n",
    "Sample  Gene1  Gene2  Gene3\n",
    "   1      10     20     15\n",
    "   2      12     24     18\n",
    "   3       8     16     12\n",
    "   4      11     22     16.5\n",
    "```\n",
    "\n",
    "**Problems**:\n",
    "- Gene1 and Gene2 are highly correlated (Gene2 ≈ 2×Gene1)\n",
    "- Gene3 is also correlated with both\n",
    "- We have redundant information across 3 dimensions\n",
    "- Variance is spread unevenly across genes\n",
    "\n",
    "### AFTER PCA (Principal Components)\n",
    "\n",
    "After standardizing (mean=0, std=1) and applying PCA:\n",
    "\n",
    "```\n",
    "Sample   PC1    PC2    PC3\n",
    "   1    -0.5    0.1    0.0\n",
    "   2     1.5   -0.2    0.0\n",
    "   3    -2.0    0.0    0.1\n",
    "   4     1.0    0.1   -0.1\n",
    "```\n",
    "\n",
    "**Explained variance**: PC1 (95%), PC2 (4%), PC3 (1%)\n",
    "\n",
    "**What Changed**:\n",
    "- **New coordinate system**: PC1, PC2, PC3 instead of Gene1, Gene2, Gene3\n",
    "- **Ordered by importance**: PC1 captures 95% of all variation\n",
    "- **Uncorrelated components**: PC1 and PC2 are orthogonal (independent)\n",
    "- **Dimensionality can be reduced**: Keep only PC1 and retain 95% of information\n",
    "\n",
    "**What Stayed the Same**:\n",
    "- **Total variance**: Sum of variance across all PCs = sum of variance across all genes\n",
    "- **Sample relationships**: Distances between samples are preserved\n",
    "- **Number of samples**: Still 4 samples (PCA doesn't add/remove samples)\n",
    "\n",
    "**What We Gained**:\n",
    "- **Compression**: 3 genes → 1 component with 95% information\n",
    "- **Denoising**: Last 2 components (5% variance) are likely noise\n",
    "- **Interpretability**: PC1 represents the dominant biological process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical_derivation",
   "metadata": {},
   "source": [
    "## Mathematical Derivation: Why Eigenvectors?\n",
    "\n",
    "### Starting from the Goal: Maximize Variance\n",
    "\n",
    "We want to find a direction **w** (unit vector) such that when we project data onto **w**, the variance is maximized.\n",
    "\n",
    "**Step 1**: Express projected data\n",
    "\n",
    "For data matrix X (centered, so mean=0), projection onto direction w is:\n",
    "```\n",
    "z = Xw\n",
    "```\n",
    "\n",
    "**Step 2**: Calculate variance of projections\n",
    "\n",
    "Variance of z:\n",
    "```\n",
    "Var(z) = (1/n) zᵀz\n",
    "       = (1/n) (Xw)ᵀ(Xw)\n",
    "       = (1/n) wᵀXᵀXw\n",
    "       = wᵀ[(1/n)XᵀX]w\n",
    "       = wᵀCw\n",
    "```\n",
    "\n",
    "where **C = (1/n)XᵀX** is the **covariance matrix**.\n",
    "\n",
    "**Step 3**: Maximize with constraint\n",
    "\n",
    "We want to maximize **wᵀCw** subject to **||w|| = 1** (unit vector constraint).\n",
    "\n",
    "Using Lagrange multipliers:\n",
    "```\n",
    "L = wᵀCw - λ(wᵀw - 1)\n",
    "```\n",
    "\n",
    "Taking derivative and setting to zero:\n",
    "```\n",
    "∂L/∂w = 2Cw - 2λw = 0\n",
    "Cw = λw\n",
    "```\n",
    "\n",
    "This is the **eigenvalue equation**! The optimal direction **w** is an **eigenvector** of C, and **λ** (the eigenvalue) equals the variance captured.\n",
    "\n",
    "**Step 4**: Ordering by importance\n",
    "\n",
    "The covariance matrix C has multiple eigenvectors. We choose:\n",
    "- **PC1** = eigenvector with largest eigenvalue λ₁ (maximum variance)\n",
    "- **PC2** = eigenvector with second-largest λ₂ (maximum remaining variance, orthogonal to PC1)\n",
    "- And so on...\n",
    "\n",
    "### Why This Formula Works\n",
    "\n",
    "**Covariance matrix C**: Encodes all pairwise correlations between features\n",
    "- Diagonal elements = variances of individual features\n",
    "- Off-diagonal elements = covariances between feature pairs\n",
    "\n",
    "**Eigenvectors of C**: Directions that are \"stretched\" by C without rotation\n",
    "- These are the natural axes of variation in the data\n",
    "- They align with the principal axes of the data ellipsoid\n",
    "\n",
    "**Eigenvalues**: Amount of variance along each eigenvector\n",
    "- Large λ = much variance = important direction\n",
    "- Small λ = little variance = less important, possibly noise\n",
    "\n",
    "### What Breaks If We Change This?\n",
    "\n",
    "- **Use correlation matrix instead of covariance**: Only valid if features are standardized (which we do)\n",
    "- **Don't center data**: Mean must be zero, otherwise variance calculation is wrong\n",
    "- **Use non-orthogonal directions**: Components would be correlated, defeating the purpose\n",
    "- **Random projections**: Might miss important variance directions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standardization_necessity",
   "metadata": {},
   "source": [
    "## Why Standardization is Critical for PCA\n",
    "\n",
    "### The Scale Problem\n",
    "\n",
    "PCA maximizes **variance**, but variance depends on measurement units:\n",
    "\n",
    "```\n",
    "Gene expression in RPKM: range 0-1000, variance ≈ 10,000\n",
    "Protein abundance in ng/mL: range 0-10, variance ≈ 5\n",
    "```\n",
    "\n",
    "Without standardization, **PC1 would be dominated by gene expression** simply because it has larger numerical values, not because it's biologically more important.\n",
    "\n",
    "### Standardization Formula\n",
    "\n",
    "For each feature:\n",
    "```\n",
    "z = (x - μ) / σ\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **μ** = mean (centering operation)\n",
    "- **σ** = standard deviation (scaling operation)\n",
    "\n",
    "**Result**: All features have mean=0 and variance=1\n",
    "\n",
    "### What Standardization Changes\n",
    "\n",
    "**Before standardization**:\n",
    "- Features have different means and variances\n",
    "- Large-magnitude features dominate PCA\n",
    "- Components reflect measurement scales, not biology\n",
    "\n",
    "**After standardization**:\n",
    "- All features contribute equally by variance\n",
    "- PCA finds directions of maximum **relative** variation\n",
    "- Components reflect biological patterns, not technical scales\n",
    "\n",
    "### What Standardization Preserves\n",
    "\n",
    "- **Correlations**: Standardization doesn't change correlation structure\n",
    "- **Sample relationships**: Relative distances between samples remain similar\n",
    "- **Biological signal**: True biological variation is preserved\n",
    "\n",
    "### When NOT to Standardize\n",
    "\n",
    "- When all features are already on the same scale (e.g., all are percentages)\n",
    "- When variance magnitude itself is biologically meaningful (rare)\n",
    "- When you want to weight high-variance features more (specific use case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invariance_properties_pca",
   "metadata": {},
   "source": [
    "## What PCA Changes and Preserves\n",
    "\n",
    "### Changes (Intentional)\n",
    "- **Feature space**: Original genes/proteins → Principal components\n",
    "- **Feature interpretation**: Each PC is a weighted combination of original features\n",
    "- **Dimensionality** (if reduced): 100 features → 10 PCs\n",
    "- **Coordinate system**: Rotated to align with variance\n",
    "- **Correlation structure**: PCs are uncorrelated by construction\n",
    "\n",
    "### Preserves (Critical)\n",
    "- **Total variance**: Sum of variances across all PCs = sum across all original features\n",
    "- **Sample relationships**: Euclidean distances between samples (if all PCs retained)\n",
    "- **Linear relationships**: PCA is a linear transformation (no information loss if all PCs kept)\n",
    "- **Number of samples**: 50 samples in → 50 samples out\n",
    "\n",
    "### Cannot Do (Limitations)\n",
    "- **Capture non-linear patterns**: PCA only finds linear combinations\n",
    "- **Separate classes automatically**: PCA is unsupervised (ignores labels)\n",
    "- **Handle categorical features**: Requires numerical continuous data\n",
    "- **Interpret components easily**: PCs are mixtures of many features\n",
    "- **Handle missing data directly**: Requires complete data matrix\n",
    "\n",
    "For non-linear patterns, use **kernel PCA, t-SNE, or UMAP**.\n",
    "For supervised dimensionality reduction, use **LDA** or **PLS**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drug_dev_context_detailed",
   "metadata": {},
   "source": [
    "## Drug Development Context: Where and Why PCA Matters\n",
    "\n",
    "### Omics Data in Drug Discovery\n",
    "\n",
    "**Genomics** (DNA sequencing):\n",
    "- Identify disease-causing mutations\n",
    "- Stratify patients by genetic background\n",
    "- PCA reveals population structure and batch effects\n",
    "\n",
    "**Transcriptomics** (RNA-seq, microarrays):\n",
    "- Measure gene expression in drug-treated vs control cells\n",
    "- Typical: 20,000 genes × 50 samples\n",
    "- PCA separates responders from non-responders\n",
    "\n",
    "**Proteomics** (mass spectrometry):\n",
    "- Quantify protein abundance changes\n",
    "- Identify biomarkers for drug efficacy\n",
    "- PCA removes instrument drift across batches\n",
    "\n",
    "**Metabolomics** (LC-MS, NMR):\n",
    "- Profile small molecule metabolites\n",
    "- Assess drug mechanism and toxicity\n",
    "- PCA detects metabolic pathway activation\n",
    "\n",
    "### How This Data is Obtained\n",
    "\n",
    "1. **Sample collection**: Patient biopsies, blood, cell cultures\n",
    "2. **Extraction**: Isolate DNA, RNA, proteins, or metabolites\n",
    "3. **Measurement**: High-throughput platforms (sequencers, mass specs, arrays)\n",
    "4. **Preprocessing**: Normalization, quality control, batch correction\n",
    "5. **Analysis**: PCA → biomarker discovery → validation\n",
    "\n",
    "### Relevance to ML Workflows\n",
    "\n",
    "**QSAR (Quantitative Structure-Activity Relationship)**:\n",
    "- Use PCA to reduce 1000s of molecular descriptors to 10-20 components\n",
    "- Build regression models: PC scores → drug potency\n",
    "- Faster training, less overfitting\n",
    "\n",
    "**ADME Prediction** (Absorption, Distribution, Metabolism, Excretion):\n",
    "- Compress physicochemical properties with PCA\n",
    "- Classify compounds as drug-like vs non-drug-like\n",
    "\n",
    "**Virtual Screening**:\n",
    "- Project chemical library into PC space\n",
    "- Find compounds similar to known actives\n",
    "- Orders of magnitude faster than full-dimensional search\n",
    "\n",
    "**Biomarker Discovery**:\n",
    "- Identify gene expression patterns that predict response\n",
    "- PC loadings reveal which genes contribute to separation\n",
    "- Validate candidate biomarkers in follow-up studies\n",
    "\n",
    "**Clinical Trial Analysis**:\n",
    "- Detect patient subgroups with different responses\n",
    "- Stratify randomization to balance groups\n",
    "- Explain variance in treatment outcomes\n",
    "\n",
    "### What Breaks Without PCA\n",
    "\n",
    "1. **Overfitting**: With 20,000 genes and 50 samples, models memorize noise\n",
    "2. **Multicollinearity**: Highly correlated features cause unstable regression coefficients\n",
    "3. **Computational cost**: Training time scales poorly with raw dimensionality\n",
    "4. **Interpretability**: Cannot visualize or inspect 20,000-dimensional space\n",
    "5. **Batch effects**: Technical variation dominates biological signal\n",
    "6. **False discoveries**: Multiple testing correction becomes impossible (20,000 tests)\n",
    "\n",
    "**Real example**: A cancer drug trial measures 15,000 genes in 80 patients. Without PCA:\n",
    "- Random forest model achieves 100% training accuracy (overfitting)\n",
    "- Validation accuracy: 50% (random guessing)\n",
    "- Cannot identify which genes matter\n",
    "\n",
    "With PCA (reduce to 20 components capturing 80% variance):\n",
    "- Model trains on meaningful variation, not noise\n",
    "- Validation accuracy: 85%\n",
    "- PC loadings point to specific pathways\n",
    "- Subsequent targeted gene validation succeeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "implementation_intro_pca",
   "metadata": {},
   "source": [
    "## Implementation in Python\n",
    "\n",
    "We'll implement PCA step-by-step using scikit-learn, simulating a realistic biomarker discovery scenario with gene expression data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1_data_generation",
   "metadata": {},
   "source": [
    "### Step 1: Generating Synthetic Biomarker Data\n",
    "\n",
    "We simulate a typical omics dataset:\n",
    "- **50 samples** (patients or biological replicates)\n",
    "- **100 features** (genes, proteins, or metabolites)\n",
    "- Values represent expression levels or concentrations\n",
    "\n",
    "The matrix structure is:\n",
    "- **Rows = samples** (observations)\n",
    "- **Columns = features** (variables)\n",
    "\n",
    "This is the standard format for ML: each row is one data point, each column is one measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3d0114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Numerical computing library\n",
    "import pandas as pd  # Data manipulation library for DataFrames\n",
    "\n",
    "# Set random seed for reproducibility - ensures same \"random\" numbers each run\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define dataset dimensions\n",
    "num_samples = 50  # Number of biological samples (patients, cell lines, etc.)\n",
    "num_features = 100  # Number of measured features (genes, proteins, metabolites)\n",
    "\n",
    "# Generate random data uniformly distributed between 0 and 10\n",
    "# Shape: (50 samples, 100 features)\n",
    "# np.random.rand generates values in [0, 1), multiplying by 10 scales to [0, 10)\n",
    "data = np.random.rand(num_samples, num_features) * 10\n",
    "\n",
    "# Convert numpy array to pandas DataFrame for easier manipulation\n",
    "# Column names: Gene_1, Gene_2, ..., Gene_100\n",
    "# List comprehension creates ['Gene_1', 'Gene_2', ...] using f-string formatting\n",
    "# i+1 because we want Gene_1 not Gene_0 for human readability\n",
    "df = pd.DataFrame(data, columns=[f'Gene_{i+1}' for i in range(num_features)])\n",
    "\n",
    "# Display first 5 rows to inspect the data structure\n",
    "# head() is a pandas method that returns the top n rows (default n=5)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2_standardization",
   "metadata": {},
   "source": [
    "### Step 2: Standardizing the Data\n",
    "\n",
    "Standardization is **mandatory** for PCA because:\n",
    "1. Different genes have different expression ranges (some 0-100, others 0-10000)\n",
    "2. PCA maximizes variance, so high-magnitude features would dominate\n",
    "3. We want to treat all features equally regardless of their measurement scale\n",
    "\n",
    "**StandardScaler** performs the transformation:\n",
    "```\n",
    "z = (x - μ) / σ\n",
    "```\n",
    "\n",
    "For each feature (column):\n",
    "- **Centering**: Subtract mean (μ) → makes mean = 0\n",
    "- **Scaling**: Divide by standard deviation (σ) → makes variance = 1\n",
    "\n",
    "After standardization:\n",
    "- All features have comparable scales\n",
    "- Covariance matrix reflects true correlations, not scale differences\n",
    "- PCA finds directions of maximum **relative** variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0188a2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  # Import standardization transformer\n",
    "\n",
    "# Initialize the scaler object\n",
    "# StandardScaler by default: centers to mean=0, scales to std=1\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit_transform: two operations in one\n",
    "# 1. fit() - calculate mean and std for each column from training data\n",
    "# 2. transform() - apply the transformation: (x - mean) / std\n",
    "# Input: DataFrame (50, 100), Output: numpy array (50, 100)\n",
    "# Each column now has mean ≈ 0 and std ≈ 1 (may have small numerical errors)\n",
    "scaled_data = scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3_pca_application",
   "metadata": {},
   "source": [
    "### Step 3: Performing PCA\n",
    "\n",
    "We apply PCA to extract the **top 10 principal components**.\n",
    "\n",
    "**What PCA does internally**:\n",
    "1. Compute covariance matrix: C = (1/n) XᵀX\n",
    "2. Find eigenvectors and eigenvalues of C\n",
    "3. Sort eigenvectors by eigenvalues (largest first)\n",
    "4. Project data onto top k eigenvectors: PC_scores = X · eigenvectors\n",
    "\n",
    "**Why n_components=10?**\n",
    "- We reduce from 100 dimensions to 10 (90% compression)\n",
    "- Typically, top 10-20 components capture 70-90% of variance in biological data\n",
    "- This is enough for visualization, clustering, and downstream modeling\n",
    "- Remaining 90 components often represent noise\n",
    "\n",
    "**Output interpretation**:\n",
    "- `principal_components`: (50 samples, 10 PCs) - the transformed data\n",
    "- Each PC is a linear combination of the original 100 genes\n",
    "- PC1 has the highest variance, PC2 has second-highest, etc.\n",
    "- PCs are **orthogonal** (uncorrelated) by construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b25d73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA  # Import PCA class\n",
    "\n",
    "# Initialize PCA with desired number of components to retain\n",
    "# n_components=10 means: keep only the top 10 principal components\n",
    "# This reduces dimensionality from 100 → 10\n",
    "pca = PCA(n_components=10)\n",
    "\n",
    "# fit_transform: learn the principal components AND project data onto them\n",
    "# 1. fit() - compute eigenvectors of covariance matrix from scaled_data\n",
    "# 2. transform() - project scaled_data onto the top 10 eigenvectors\n",
    "# Input: (50 samples, 100 features), Output: (50 samples, 10 PCs)\n",
    "# Each row is one sample, now represented by its coordinates in PC space\n",
    "principal_components = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Convert to DataFrame for interpretability\n",
    "# Column names: PC_1, PC_2, ..., PC_10\n",
    "# PC_1 is the direction of maximum variance, PC_2 is second, etc.\n",
    "pc_df = pd.DataFrame(principal_components, columns=[f'PC_{i+1}' for i in range(10)])\n",
    "\n",
    "# Display first 5 samples in the new PC coordinate system\n",
    "# These values are the \"scores\" - projections of samples onto principal components\n",
    "pc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4_variance_analysis",
   "metadata": {},
   "source": [
    "### Step 4: Explained Variance Analysis\n",
    "\n",
    "**Explained variance ratio** answers: \"What fraction of total variance does each PC capture?\"\n",
